{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - What is KV caching and Why do we need it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KV caching is a technique used in autoregressive models like GPT to speed up text generation by storing the computed key-value pairs from the attention layers during inference. Instead of recalculating the attention for every token in the sequence, the model retrieves the cached keys and values for previously generated tokens, only computing the attention for the new token. This reduces redundant calculations, significantly improving the speed and efficiency of generating long sequences, especially in memory-constrained environments. KV caching is essential for tasks like text generation, where tokens are produced one by one, but it is not necessary for tasks that process the entire input sequence at once, like classification or translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 -Is it due to autogressiveness of LLMs while inference ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KV caching is a technique used in autoregressive models like GPT to speed up text generation by storing the computed key-value pairs from the attention layers during inference. Instead of recalculating the attention for every token in the sequence, the model retrieves the cached keys and values for previously generated tokens, only computing the attention for the new token. This reduces redundant calculations, significantly improving the speed and efficiency of generating long sequences, especially in memory-constrained environments. KV caching is essential for tasks like text generation, where tokens are produced one by one, but it is not necessary for tasks that process the entire input sequence at once, like classification or translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - is it enabled default in model loaded via using huggingface ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, KV caching is not enabled by default in models from Hugging Face. You have to explicitly enable it when performing inference, typically during the text generation process.\n",
    "\n",
    "For example, when using models like GPT-2 or GPT-3 for text generation, you need to set use_cache=True in the generation method. Here’s an example of how to enable KV caching during generation:\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, use_cache=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - if you're using the model for other tasks (like classification or sequence labeling), KV caching is not needed and is not enabled, as those tasks process the entire sequence at once without token-by-token generation. Explain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why KV Caching Isn’t Used Here\n",
    "KV caching is specifically designed for autoregressive generation tasks, where the model needs to generate text token by token. As each new token is generated, the model needs to remember all the previous tokens and their attention representations.\n",
    "In contrast, for classification or sequence labeling, the entire sequence is processed in one go, so there's no need for caching of intermediate results for each token. All tokens are available to the model in the same pass, making it unnecessary to store and reuse past computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
